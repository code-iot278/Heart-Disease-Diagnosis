# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lT6_2FDBTHrZ68tkcKalBFS-zoMyfpCc

# **Access Drive File**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Federated Multimodal Explainable AI Framework for Early and Privacy-Preserving Heart Disease Diagnosis**

# **ECG Image**

# **Anisotropic Diffusion**
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ===============================
# Anisotropic Diffusion Function
# ===============================
def anisotropic_diffusion_color(img, num_iter=10, kappa=30, gamma=0.1, option=1):
    img = img.astype(np.float32)
    out = img.copy()

    for channel in range(3):  # Process each RGB channel
        channel_img = img[:, :, channel]
        for i in range(num_iter):
            north = np.zeros_like(channel_img)
            south = np.zeros_like(channel_img)
            east  = np.zeros_like(channel_img)
            west  = np.zeros_like(channel_img)

            north[1:, :] = channel_img[1:, :] - channel_img[:-1, :]
            south[:-1, :] = channel_img[:-1, :] - channel_img[1:, :]
            east[:, :-1] = channel_img[:, :-1] - channel_img[:, 1:]
            west[:, 1:] = channel_img[:, 1:] - channel_img[:, :-1]

            if option == 1:
                c_n = np.exp(-(north/kappa)**2)
                c_s = np.exp(-(south/kappa)**2)
                c_e = np.exp(-(east/kappa)**2)
                c_w = np.exp(-(west/kappa)**2)
            elif option == 2:
                c_n = 1 / (1 + (north/kappa)**2)
                c_s = 1 / (1 + (south/kappa)**2)
                c_e = 1 / (1 + (east/kappa)**2)
                c_w = 1 / (1 + (west/kappa)**2)

            channel_img += gamma * (c_n*north + c_s*south + c_e*east + c_w*west)

        out[:, :, channel] = channel_img

    return np.clip(out, 0, 255).astype(np.uint8)

# ===============================
# Function to process folder recursively and save output
# ===============================
def process_and_save_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Compute relative path to maintain folder structure
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load color image
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply anisotropic diffusion
                diffused = anisotropic_diffusion_color(img_rgb, num_iter=15, kappa=20, gamma=0.15, option=1)

                # Display original and diffused
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(diffused)
                plt.title('Diffused')
                plt.axis('off')

                plt.show()

                # Save diffused image
                save_path = os.path.join(save_dir, file)
                diffused_bgr = cv2.cvtColor(diffused, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, diffused_bgr)
                print(f"Saved: {save_path}")

# ===============================
# Main folder paths
# ===============================
input_folder = '/content/drive/MyDrive/Colab Notebooks/heart/archive (93)/ecg_data_new_version/ecg data new version'
output_folder = '/content/drive/MyDrive/Colab Notebooks/heart/output_diffused'

process_and_save_recursive(input_folder, output_folder)

"""# **CLAHE**"""

import os
import cv2
import matplotlib.pyplot as plt

# ==========================
# CLAHE Function for Color Images
# ==========================
def apply_clahe_color(img, clip_limit=2.0, tile_grid_size=(8,8)):
    """
    Apply CLAHE to a color image (RGB).
    """
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    l_clahe = clahe.apply(l)
    lab_clahe = cv2.merge((l_clahe, a, b))
    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)
    return img_clahe

# ==========================
# Process Folder Recursively
# ==========================
def process_and_save_clahe_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Maintain folder structure in output
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load image in color
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply CLAHE
                clahe_img = apply_clahe_color(img_rgb, clip_limit=2.0, tile_grid_size=(8,8))

                # Display Original and CLAHE images
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(clahe_img)
                plt.title('CLAHE')
                plt.axis('off')

                plt.show()

                # Save CLAHE image
                save_path = os.path.join(save_dir, file)
                clahe_bgr = cv2.cvtColor(clahe_img, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, clahe_bgr)
                print(f"Saved: {save_path}")

# ==========================
# Main Folder Paths
# ==========================
input_folder = '/content/drive/MyDrive/Colab Notebooks/heart/output_diffused'
output_folder = '/content/drive/MyDrive/Colab Notebooks/heart/output_clahe'

process_and_save_clahe_recursive(input_folder, output_folder)

"""# **(ECG images only) Wavelet + CWT**"""

import cv2
import numpy as np
import pywt
import matplotlib.pyplot as plt
import os

# =========================
# 1. Main input and output folders
# =========================
main_folder = '/content/drive/MyDrive/Colab Notebooks/heart/output_clahe'
output_folder = '/content/drive/MyDrive/Colab Notebooks/heart/aggregated_cwtfinal'

os.makedirs(output_folder, exist_ok=True)

# Loop through subfolders
for subfolder in os.listdir(main_folder):
    subfolder_path = os.path.join(main_folder, subfolder)
    if not os.path.isdir(subfolder_path):
        continue

    output_subfolder = os.path.join(output_folder, subfolder)
    os.makedirs(output_subfolder, exist_ok=True)

    for file in os.listdir(subfolder_path):
        if not file.lower().endswith(('.jpg', '.png')):
            continue

        img_path = os.path.join(subfolder_path, file)

        # =========================
        # READ ORIGINAL COLOR IMAGE
        # =========================
        img_color = cv2.imread(img_path)   # BGR
        if img_color is None:
            continue

        img_rgb = cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB)
        img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)

        print(f"Processing: {img_path}")

        # =========================
        # 2. Aggregated CWT (using grayscale)
        # =========================
        scales = np.arange(1, 128)
        cwt_sum = np.zeros((len(scales), img_gray.shape[1]))

        for r in range(0, img_gray.shape[0], 5):
            coef, _ = pywt.cwt(img_gray[r, :], scales, 'morl')
            cwt_sum += np.abs(coef)

        # Normalize
        cwt_norm = (cwt_sum - cwt_sum.min()) / (cwt_sum.max() - cwt_sum.min())

        # =========================
        # 3. DISPLAY INPUT & OUTPUT
        # =========================
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        plt.imshow(img_rgb)
        plt.title("Input Image (Original Color)")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(cwt_norm, cmap='jet', aspect='auto', origin='upper')
        plt.title("Aggregated CWT")
        plt.xlabel("Time")
        plt.ylabel("Scale")

        plt.tight_layout()
        plt.show()

        # =========================
        # 4. SAVE FULL CWT IMAGE
        # =========================
        output_path = os.path.join(
            output_subfolder,
            file.replace('.jpg', '_aggCWT.png').replace('.png', '_aggCWT.png')
        )

        plt.figure(figsize=(10, 6))
        plt.imshow(cwt_norm, cmap='jet', aspect='auto', origin='upper')
        plt.axis('off')
        plt.tight_layout(pad=0)
        plt.savefig(output_path, dpi=300, bbox_inches='tight', pad_inches=0)
        plt.close()

        print(f"Saved FULL Aggregated CWT: {output_path}")

"""# **segmentation**

# **Attention UNet++ Model**
"""

# ============================================================
# FULL PIPELINE:
# Red + Yellow Segmentation → Save Segmented Image Folder-wise
# ============================================================

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.nn.functional as F

# =========================
# Attention Block
# =========================
class AttentionBlock(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super().__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        g1 = F.interpolate(g1, size=x.shape[2:], mode='bilinear', align_corners=True)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

# =========================
# Convolution Block
# =========================
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

# =========================
# Attention UNet++ (Defined for completeness)
# =========================
class AttentionUNetPlusPlus(nn.Module):
    def __init__(self, in_ch=3, out_ch=1, filters=[64,128,256,512]):
        super().__init__()
        self.pool = nn.MaxPool2d(2,2)

        self.conv0_0 = ConvBlock(in_ch, filters[0])
        self.conv1_0 = ConvBlock(filters[0], filters[1])
        self.conv2_0 = ConvBlock(filters[1], filters[2])
        self.conv3_0 = ConvBlock(filters[2], filters[3])

        self.att0_1 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_2 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_3 = AttentionBlock(filters[1], filters[0], filters[0]//2)

        self.conv0_1 = ConvBlock(filters[0]*2, filters[0])
        self.conv0_2 = ConvBlock(filters[0]*3, filters[0])
        self.conv0_3 = ConvBlock(filters[0]*4, filters[0])

        self.final = nn.Conv2d(filters[0], out_ch, 1)

    def forward(self, x):
        x0_0 = self.conv0_0(x)
        x1_0 = self.conv1_0(self.pool(x0_0))

        att0_1 = self.att0_1(x1_0, x0_0)
        x0_1 = self.conv0_1(torch.cat([x0_0, att0_1], dim=1))

        att0_2 = self.att0_2(x1_0, x0_1)
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, att0_2], dim=1))

        att0_3 = self.att0_3(x1_0, x0_2)
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, att0_3], dim=1))

        return torch.sigmoid(self.final(x0_3))

# ============================================================
# INPUT / OUTPUT FOLDERS
# ============================================================
input_folder = "/content/drive/MyDrive/Colab Notebooks/heart/aggregated_cwtfinal/abnormal_heartbeat_ecg_images"
output_folder = "/content/drive/MyDrive/Colab Notebooks/processed1/segmented_only"
os.makedirs(output_folder, exist_ok=True)

# ============================================================
# PROCESS IMAGES
# ============================================================
for filename in os.listdir(input_folder):
    if filename.lower().endswith((".png", ".jpg", ".jpeg")):

        image_path = os.path.join(input_folder, filename)

        # ----------------------------
        # Load & resize
        # ----------------------------
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (256, 256))

        # ----------------------------
        # HSV conversion
        # ----------------------------
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

        # ----------------------------
        # Yellow mask
        # ----------------------------
        lower_yellow = np.array([20, 100, 100])
        upper_yellow = np.array([35, 255, 255])
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)

        # ----------------------------
        # Red mask (two ranges)
        # ----------------------------
        lower_red1 = np.array([0, 120, 100])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([170, 120, 100])
        upper_red2 = np.array([180, 255, 255])

        red_mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        red_mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        red_mask = cv2.bitwise_or(red_mask1, red_mask2)

        # ----------------------------
        # Combined mask
        # ----------------------------
        roi_mask = cv2.bitwise_or(yellow_mask, red_mask)
        roi_mask = roi_mask // 255

        # Morphological cleanup
        kernel = np.ones((3,3), np.uint8)
        roi_mask = cv2.morphologyEx(roi_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)
        roi_mask = cv2.morphologyEx(roi_mask, cv2.MORPH_CLOSE, kernel)

        # ----------------------------
        # Segmented image only
        # ----------------------------
        segmented_only = image * roi_mask[:, :, np.newaxis]

        # ----------------------------
        # Save segmented image
        # ----------------------------
        save_path = os.path.join(output_folder, filename)
        cv2.imwrite(save_path, cv2.cvtColor(segmented_only, cv2.COLOR_RGB2BGR))

        # ----------------------------
        # Display (optional)
        # ----------------------------
        plt.figure(figsize=(12,4))

        plt.subplot(1,3,1)
        plt.title("Input")
        plt.imshow(image)
        plt.axis("off")

        plt.subplot(1,3,2)
        plt.title("Mask")
        plt.imshow(roi_mask, cmap="gray")
        plt.axis("off")

        plt.subplot(1,3,3)
        plt.title("Segmented Only")
        plt.imshow(segmented_only)
        plt.axis("off")

        plt.tight_layout()
        plt.show()

print("✅ Segmented images saved successfully!")

"""# **Registration**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os

# ====================================================
# 0. INPUT / OUTPUT FOLDERS
# ====================================================
input_folder  = "/content/drive/MyDrive/Colab Notebooks/processed1/segmented_only"
output_folder = "/content/drive/MyDrive/Colab Notebooks/registered_only45"
os.makedirs(output_folder, exist_ok=True)

# ====================================================
# 1. SELECT REFERENCE IMAGE (ONLY FILENAME)
# ====================================================
ref_image_name = "HB(1)_aggCWT_aggCWT.png"
ref_path = os.path.join(input_folder, ref_image_name)

# Load reference (color)
ref_color = cv2.imread(ref_path)
ref_color = cv2.cvtColor(ref_color, cv2.COLOR_BGR2RGB)
ref_color = cv2.resize(ref_color, (256, 256))

# Convert reference to grayscale (ECC only)
ref_gray = cv2.cvtColor(ref_color, cv2.COLOR_RGB2GRAY)

# ====================================================
# 2. REGISTRATION PARAMETERS
# ====================================================
warp_mode = cv2.MOTION_AFFINE
criteria = (
    cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,
    5000,
    1e-6
)

# ====================================================
# 3. PROCESS EACH IMAGE
# ====================================================
for filename in sorted(os.listdir(input_folder)):

    if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    if filename == ref_image_name:
        continue

    print(f"Registering: {filename}")

    img_path = os.path.join(input_folder, filename)

    # ----------------------------
    # Load moving image (color)
    # ----------------------------
    mov_color = cv2.imread(img_path)
    mov_color = cv2.cvtColor(mov_color, cv2.COLOR_BGR2RGB)
    mov_color = cv2.resize(mov_color, (256, 256))

    # Convert to grayscale (ECC only)
    mov_gray = cv2.cvtColor(mov_color, cv2.COLOR_RGB2GRAY)

    # ----------------------------
    # Initialize warp matrix
    # ----------------------------
    warp_matrix = np.eye(2, 3, dtype=np.float32)

    # ----------------------------
    # ECC Registration
    # ----------------------------
    try:
        cc, warp_matrix = cv2.findTransformECC(
            ref_gray,
            mov_gray,
            warp_matrix,
            warp_mode,
            criteria
        )
    except cv2.error:
        print(f"⚠️ Registration failed for {filename}")
        continue

    # ----------------------------
    # Apply warp to COLOR image
    # ----------------------------
    registered_color = cv2.warpAffine(
        mov_color,
        warp_matrix,
        (ref_color.shape[1], ref_color.shape[0]),
        flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP
    )

    # ----------------------------
    # Save registered image
    # ----------------------------
    save_path = os.path.join(output_folder, filename)
    cv2.imwrite(save_path, cv2.cvtColor(registered_color, cv2.COLOR_RGB2BGR))

    # ====================================================
    # DISPLAY EACH RESULT
    # ====================================================
    plt.figure(figsize=(15,5))

    plt.subplot(1,3,1)
    plt.title("Reference Image")
    plt.imshow(ref_color)
    plt.axis("off")

    plt.subplot(1,3,2)
    plt.title(f"Moving Image: {filename}")
    plt.imshow(mov_color)
    plt.axis("off")

    plt.subplot(1,3,3)
    plt.title("Registered Image")
    plt.imshow(registered_color)
    plt.axis("off")

    plt.tight_layout()
    plt.show()

print("✅ Batch registration + display completed successfully!")

"""# **CT Image**

# **Anisotropic Diffusion**
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ===============================
# Anisotropic Diffusion Function
# ===============================
def anisotropic_diffusion_color(img, num_iter=10, kappa=30, gamma=0.1, option=1):
    img = img.astype(np.float32)
    out = img.copy()

    for channel in range(3):  # Process each RGB channel
        channel_img = img[:, :, channel]
        for i in range(num_iter):
            north = np.zeros_like(channel_img)
            south = np.zeros_like(channel_img)
            east  = np.zeros_like(channel_img)
            west  = np.zeros_like(channel_img)

            north[1:, :] = channel_img[1:, :] - channel_img[:-1, :]
            south[:-1, :] = channel_img[:-1, :] - channel_img[1:, :]
            east[:, :-1] = channel_img[:, :-1] - channel_img[:, 1:]
            west[:, 1:] = channel_img[:, 1:] - channel_img[:, :-1]

            if option == 1:
                c_n = np.exp(-(north/kappa)**2)
                c_s = np.exp(-(south/kappa)**2)
                c_e = np.exp(-(east/kappa)**2)
                c_w = np.exp(-(west/kappa)**2)
            elif option == 2:
                c_n = 1 / (1 + (north/kappa)**2)
                c_s = 1 / (1 + (south/kappa)**2)
                c_e = 1 / (1 + (east/kappa)**2)
                c_w = 1 / (1 + (west/kappa)**2)

            channel_img += gamma * (c_n*north + c_s*south + c_e*east + c_w*west)

        out[:, :, channel] = channel_img

    return np.clip(out, 0, 255).astype(np.uint8)

# ===============================
# Function to process folder recursively and save output
# ===============================
def process_and_save_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Compute relative path to maintain folder structure
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load color image
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply anisotropic diffusion
                diffused = anisotropic_diffusion_color(img_rgb, num_iter=15, kappa=20, gamma=0.15, option=1)

                # Display original and diffused
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(diffused)
                plt.title('Diffused')
                plt.axis('off')

                plt.show()

                # Save diffused image
                save_path = os.path.join(save_dir, file)
                diffused_bgr = cv2.cvtColor(diffused, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, diffused_bgr)
                print(f"Saved: {save_path}")

# ===============================
# Main folder paths
# ===============================
input_folder = '/content/drive/MyDrive/Colab Notebooks/archive (97)/data/train'
output_folder = '/content/drive/MyDrive/Colab Notebooks/archive (97)/output_diffused'

process_and_save_recursive(input_folder, output_folder)

"""# **CLAHE**"""

import os
import cv2
import matplotlib.pyplot as plt

# ==========================
# CLAHE Function for Color Images
# ==========================
def apply_clahe_color(img, clip_limit=2.0, tile_grid_size=(8,8)):
    """
    Apply CLAHE to a color image (RGB).
    """
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    l_clahe = clahe.apply(l)
    lab_clahe = cv2.merge((l_clahe, a, b))
    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)
    return img_clahe

# ==========================
# Process Folder Recursively
# ==========================
def process_and_save_clahe_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Maintain folder structure in output
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load image in color
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply CLAHE
                clahe_img = apply_clahe_color(img_rgb, clip_limit=2.0, tile_grid_size=(8,8))

                # Display Original and CLAHE images
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(clahe_img)
                plt.title('CLAHE')
                plt.axis('off')

                plt.show()

                # Save CLAHE image
                save_path = os.path.join(save_dir, file)
                clahe_bgr = cv2.cvtColor(clahe_img, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, clahe_bgr)
                print(f"Saved: {save_path}")

# ==========================
# Main Folder Paths
# ==========================
input_folder = '/content/drive/MyDrive/Colab Notebooks/archive (97)/output_diffused'
output_folder = '/content/drive/MyDrive/Colab Notebooks/archive (97)/output_clahe'

process_and_save_clahe_recursive(input_folder, output_folder)

"""# **segmentation**
# **Attention UNet++ Model**
"""

# ============================================================
# FULL PIPELINE:
# WHITE Color Segmentation → Save Segmented Image Folder-wise
# ============================================================

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
# =========================
# Attention Block
# =========================
class AttentionBlock(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super().__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        g1 = F.interpolate(g1, size=x.shape[2:], mode='bilinear', align_corners=True)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

# =========================
# Convolution Block
# =========================
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

# =========================
# Attention UNet++ (Defined for completeness)
# =========================
class AttentionUNetPlusPlus(nn.Module):
    def __init__(self, in_ch=3, out_ch=1, filters=[64,128,256,512]):
        super().__init__()
        self.pool = nn.MaxPool2d(2,2)

        self.conv0_0 = ConvBlock(in_ch, filters[0])
        self.conv1_0 = ConvBlock(filters[0], filters[1])
        self.conv2_0 = ConvBlock(filters[1], filters[2])
        self.conv3_0 = ConvBlock(filters[2], filters[3])

        self.att0_1 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_2 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_3 = AttentionBlock(filters[1], filters[0], filters[0]//2)

        self.conv0_1 = ConvBlock(filters[0]*2, filters[0])
        self.conv0_2 = ConvBlock(filters[0]*3, filters[0])
        self.conv0_3 = ConvBlock(filters[0]*4, filters[0])

        self.final = nn.Conv2d(filters[0], out_ch, 1)

    def forward(self, x):
        x0_0 = self.conv0_0(x)
        x1_0 = self.conv1_0(self.pool(x0_0))

        att0_1 = self.att0_1(x1_0, x0_0)
        x0_1 = self.conv0_1(torch.cat([x0_0, att0_1], dim=1))

        att0_2 = self.att0_2(x1_0, x0_1)
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, att0_2], dim=1))

        att0_3 = self.att0_3(x1_0, x0_2)
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, att0_3], dim=1))

        return torch.sigmoid(self.final(x0_3))

# ============================================================
# INPUT / OUTPUT FOLDERS
# ============================================================
input_folder = "/content/drive/MyDrive/Colab Notebooks/archive (97)/output_clahe/100094/image"
output_folder = "/content/drive/MyDrive/Colab Notebooks/archive (97)/segmented_only"
os.makedirs(output_folder, exist_ok=True)

# ============================================================
# PROCESS IMAGES
# ============================================================
for filename in sorted(os.listdir(input_folder)):

    if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    image_path = os.path.join(input_folder, filename)

    # ----------------------------
    # Load & resize
    # ----------------------------
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (256, 256))

    # ----------------------------
    # Convert to HSV
    # ----------------------------
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

    # ----------------------------
    # WHITE color mask
    # (Low saturation, high value)
    # ----------------------------
    lower_white = np.array([0, 0, 170])
    upper_white = np.array([180, 50, 255])
    white_mask = cv2.inRange(hsv, lower_white, upper_white)

    # Convert mask to 0/1
    roi_mask = white_mask // 255

    # ----------------------------
    # Morphological cleanup
    # ----------------------------
    kernel = np.ones((3,3), np.uint8)
    roi_mask = cv2.morphologyEx(roi_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)
    roi_mask = cv2.morphologyEx(roi_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)

    # ----------------------------
    # Segmented WHITE region only
    # ----------------------------
    segmented_only = image * roi_mask[:, :, np.newaxis]

    # ----------------------------
    # Save segmented image
    # ----------------------------
    save_path = os.path.join(output_folder, filename)
    cv2.imwrite(save_path, cv2.cvtColor(segmented_only, cv2.COLOR_RGB2BGR))

    # ----------------------------
    # Display (optional)
    # ----------------------------
    plt.figure(figsize=(12,4))

    plt.subplot(1,3,1)
    plt.title("Input Image")
    plt.imshow(image)
    plt.axis("off")

    plt.subplot(1,3,2)
    plt.title("White Mask")
    plt.imshow(roi_mask, cmap="gray")
    plt.axis("off")

    plt.subplot(1,3,3)
    plt.title("White Segmented Region")
    plt.imshow(segmented_only)
    plt.axis("off")

    plt.tight_layout()
    plt.show()

print("✅ WHITE segmented images saved successfully!")

"""# **Registration**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os

# ====================================================
# 0. INPUT / OUTPUT FOLDERS
# ====================================================
input_folder  = "/content/drive/MyDrive/Colab Notebooks/archive (97)/segmented_only"
output_folder = "/content/drive/MyDrive/Colab Notebooks/archive (97)/registered_only2"
os.makedirs(output_folder, exist_ok=True)

# ====================================================
# 1. SELECT REFERENCE IMAGE (ONLY FILENAME)
# ====================================================
ref_image_name = "/content/drive/MyDrive/Colab Notebooks/archive (97)/segmented_only/1-067.png"
ref_path = os.path.join(input_folder, ref_image_name)

# Load reference (color)
ref_color = cv2.imread(ref_path)
ref_color = cv2.cvtColor(ref_color, cv2.COLOR_BGR2RGB)
ref_color = cv2.resize(ref_color, (256, 256))

# Convert reference to grayscale (ECC only)
ref_gray = cv2.cvtColor(ref_color, cv2.COLOR_RGB2GRAY)

# ====================================================
# 2. REGISTRATION PARAMETERS
# ====================================================
warp_mode = cv2.MOTION_AFFINE
criteria = (
    cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,
    5000,
    1e-6
)

# ====================================================
# 3. PROCESS EACH IMAGE
# ====================================================
for filename in sorted(os.listdir(input_folder)):

    if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    if filename == ref_image_name:
        continue

    print(f"Registering: {filename}")

    img_path = os.path.join(input_folder, filename)

    # ----------------------------
    # Load moving image (color)
    # ----------------------------
    mov_color = cv2.imread(img_path)
    mov_color = cv2.cvtColor(mov_color, cv2.COLOR_BGR2RGB)
    mov_color = cv2.resize(mov_color, (256, 256))

    # Convert to grayscale (ECC only)
    mov_gray = cv2.cvtColor(mov_color, cv2.COLOR_RGB2GRAY)

    # ----------------------------
    # Initialize warp matrix
    # ----------------------------
    warp_matrix = np.eye(2, 3, dtype=np.float32)

    # ----------------------------
    # ECC Registration
    # ----------------------------
    try:
        cc, warp_matrix = cv2.findTransformECC(
            ref_gray,
            mov_gray,
            warp_matrix,
            warp_mode,
            criteria
        )
    except cv2.error:
        print(f"⚠️ Registration failed for {filename}")
        continue

    # ----------------------------
    # Apply warp to COLOR image
    # ----------------------------
    registered_color = cv2.warpAffine(
        mov_color,
        warp_matrix,
        (ref_color.shape[1], ref_color.shape[0]),
        flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP
    )

    # ----------------------------
    # Save registered image
    # ----------------------------
    save_path = os.path.join(output_folder, filename)
    cv2.imwrite(save_path, cv2.cvtColor(registered_color, cv2.COLOR_RGB2BGR))

    # ====================================================
    # DISPLAY EACH RESULT
    # ====================================================
    plt.figure(figsize=(15,5))

    plt.subplot(1,3,1)
    plt.title("Reference Image")
    plt.imshow(ref_color)
    plt.axis("off")

    plt.subplot(1,3,2)
    plt.title(f"Moving Image: {filename}")
    plt.imshow(mov_color)
    plt.axis("off")

    plt.subplot(1,3,3)
    plt.title("Registered Image")
    plt.imshow(registered_color)
    plt.axis("off")

    plt.tight_layout()
    plt.show()

print("✅ Batch registration + display completed successfully!")

"""# **MRI Image**

# **Anisotropic Diffusion**
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# ===============================
# Anisotropic Diffusion Function
# ===============================
def anisotropic_diffusion_color(img, num_iter=10, kappa=30, gamma=0.1, option=1):
    img = img.astype(np.float32)
    out = img.copy()

    for channel in range(3):  # Process each RGB channel
        channel_img = img[:, :, channel]
        for i in range(num_iter):
            north = np.zeros_like(channel_img)
            south = np.zeros_like(channel_img)
            east  = np.zeros_like(channel_img)
            west  = np.zeros_like(channel_img)

            north[1:, :] = channel_img[1:, :] - channel_img[:-1, :]
            south[:-1, :] = channel_img[:-1, :] - channel_img[1:, :]
            east[:, :-1] = channel_img[:, :-1] - channel_img[:, 1:]
            west[:, 1:] = channel_img[:, 1:] - channel_img[:, :-1]

            if option == 1:
                c_n = np.exp(-(north/kappa)**2)
                c_s = np.exp(-(south/kappa)**2)
                c_e = np.exp(-(east/kappa)**2)
                c_w = np.exp(-(west/kappa)**2)
            elif option == 2:
                c_n = 1 / (1 + (north/kappa)**2)
                c_s = 1 / (1 + (south/kappa)**2)
                c_e = 1 / (1 + (east/kappa)**2)
                c_w = 1 / (1 + (west/kappa)**2)

            channel_img += gamma * (c_n*north + c_s*south + c_e*east + c_w*west)

        out[:, :, channel] = channel_img

    return np.clip(out, 0, 255).astype(np.uint8)

# ===============================
# Function to process folder recursively and save output
# ===============================
def process_and_save_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Compute relative path to maintain folder structure
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load color image
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply anisotropic diffusion
                diffused = anisotropic_diffusion_color(img_rgb, num_iter=15, kappa=20, gamma=0.15, option=1)

                # Display original and diffused
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(diffused)
                plt.title('Diffused')
                plt.axis('off')

                plt.show()

                # Save diffused image
                save_path = os.path.join(save_dir, file)
                diffused_bgr = cv2.cvtColor(diffused, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, diffused_bgr)
                print(f"Saved: {save_path}")

# ===============================
# Main folder paths
# ===============================
input_folder = '/content/drive/MyDrive/Colab Notebooks/archive (99)'
output_folder = '/content/drive/MyDrive/Colab Notebooks/mri_output_diffused'

process_and_save_recursive(input_folder, output_folder)

"""# **CLAHE**"""

import os
import cv2
import matplotlib.pyplot as plt

# ==========================
# CLAHE Function for Color Images
# ==========================
def apply_clahe_color(img, clip_limit=2.0, tile_grid_size=(8,8)):
    """
    Apply CLAHE to a color image (RGB).
    """
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    l_clahe = clahe.apply(l)
    lab_clahe = cv2.merge((l_clahe, a, b))
    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)
    return img_clahe

# ==========================
# Process Folder Recursively
# ==========================
def process_and_save_clahe_recursive(input_folder, output_folder):
    for root, dirs, files in os.walk(input_folder):
        # Maintain folder structure in output
        rel_path = os.path.relpath(root, input_folder)
        save_dir = os.path.join(output_folder, rel_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)

                # Load image in color
                img = cv2.imread(file_path)
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply CLAHE
                clahe_img = apply_clahe_color(img_rgb, clip_limit=2.0, tile_grid_size=(8,8))

                # Display Original and CLAHE images
                plt.figure(figsize=(10,5))
                plt.suptitle(file_path)

                plt.subplot(1,2,1)
                plt.imshow(img_rgb)
                plt.title('Original')
                plt.axis('off')

                plt.subplot(1,2,2)
                plt.imshow(clahe_img)
                plt.title('CLAHE')
                plt.axis('off')

                plt.show()

                # Save CLAHE image
                save_path = os.path.join(save_dir, file)
                clahe_bgr = cv2.cvtColor(clahe_img, cv2.COLOR_RGB2BGR)
                cv2.imwrite(save_path, clahe_bgr)
                print(f"Saved: {save_path}")

# ==========================
# Main Folder Paths
# ==========================
input_folder = '/content/drive/MyDrive/Colab Notebooks/mri_output_diffused'
output_folder = '/content/drive/MyDrive/Colab Notebooks/output_clahe'

process_and_save_clahe_recursive(input_folder, output_folder)

"""# **segmentation**
# **Attention UNet++ Model**
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
# ============================================================
# FULL PIPELINE:
# WHITE Color Segmentation → Save Segmented Image Folder-wise
# ============================================================

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
# =========================
# Attention Block
# =========================
class AttentionBlock(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super().__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1),
            nn.BatchNorm2d(F_int)
        )
        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        g1 = F.interpolate(g1, size=x.shape[2:], mode='bilinear', align_corners=True)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

# =========================
# Convolution Block
# =========================
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

# =========================
# Attention UNet++ (Defined for completeness)
# =========================
class AttentionUNetPlusPlus(nn.Module):
    def __init__(self, in_ch=3, out_ch=1, filters=[64,128,256,512]):
        super().__init__()
        self.pool = nn.MaxPool2d(2,2)

        self.conv0_0 = ConvBlock(in_ch, filters[0])
        self.conv1_0 = ConvBlock(filters[0], filters[1])
        self.conv2_0 = ConvBlock(filters[1], filters[2])
        self.conv3_0 = ConvBlock(filters[2], filters[3])

        self.att0_1 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_2 = AttentionBlock(filters[1], filters[0], filters[0]//2)
        self.att0_3 = AttentionBlock(filters[1], filters[0], filters[0]//2)

        self.conv0_1 = ConvBlock(filters[0]*2, filters[0])
        self.conv0_2 = ConvBlock(filters[0]*3, filters[0])
        self.conv0_3 = ConvBlock(filters[0]*4, filters[0])

        self.final = nn.Conv2d(filters[0], out_ch, 1)

    def forward(self, x):
        x0_0 = self.conv0_0(x)
        x1_0 = self.conv1_0(self.pool(x0_0))

        att0_1 = self.att0_1(x1_0, x0_0)
        x0_1 = self.conv0_1(torch.cat([x0_0, att0_1], dim=1))

        att0_2 = self.att0_2(x1_0, x0_1)
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, att0_2], dim=1))

        att0_3 = self.att0_3(x1_0, x0_2)
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, att0_3], dim=1))

        return torch.sigmoid(self.final(x0_3))
# ============================================================
# INPUT / OUTPUT MAIN FOLDERS
# ============================================================
input_main_folder  = "/content/drive/MyDrive/Colab Notebooks/output_clahe"
output_main_folder = "/content/drive/MyDrive/Colab Notebooks/heart_segmented"

os.makedirs(output_main_folder, exist_ok=True)

# ============================================================
# WALK THROUGH ALL SUBFOLDERS & IMAGES
# ============================================================
for root, dirs, files in os.walk(input_main_folder):

    relative_path = os.path.relpath(root, input_main_folder)
    output_root = os.path.join(output_main_folder, relative_path)
    os.makedirs(output_root, exist_ok=True)

    for filename in files:
        if filename.lower().endswith((".png", ".jpg", ".jpeg")):

            image_path = os.path.join(root, filename)

            # ===============================
            # Load image (grayscale)
            # ===============================
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            image = cv2.resize(image, (256, 256))

            # ===============================
            # CLAHE
            # ===============================
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(image)

            # ===============================
            # Blur
            # ===============================
            blur = cv2.GaussianBlur(enhanced, (5,5), 0)

            # ===============================
            # Otsu Threshold
            # ===============================
            _, binary = cv2.threshold(
                blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
            )

            # ===============================
            # Morphology
            # ===============================
            kernel = np.ones((5,5), np.uint8)
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)

            # ===============================
            # Largest Connected Component
            # ===============================
            num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary)

            heart_mask = np.zeros_like(binary)
            if num_labels > 1:
                largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
                heart_mask[labels == largest_label] = 255
            else:
                heart_mask = binary

            # ===============================
            # Segmented image
            # ===============================
            segmented = cv2.bitwise_and(image, image, mask=heart_mask)

            # ===============================
            # Save output
            # ===============================
            save_path = os.path.join(output_root, filename)
            cv2.imwrite(save_path, segmented)

            # ===============================
            # DISPLAY RESULTS
            # ===============================
            plt.figure(figsize=(12,4))

            plt.subplot(1,3,1)
            plt.title("Input")
            plt.imshow(image, cmap='gray')
            plt.axis("off")

            plt.subplot(1,3,2)
            plt.title("Heart Mask")
            plt.imshow(heart_mask, cmap='gray')
            plt.axis("off")

            plt.subplot(1,3,3)
            plt.title("Segmented Heart")
            plt.imshow(segmented, cmap='gray')
            plt.axis("off")

            plt.suptitle(filename, fontsize=12, fontweight='bold')
            plt.tight_layout()
            plt.show()

print("✅ Input, mask, and segmented images displayed and saved successfully!")

"""# **Registration**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os

# ====================================================
# 0. INPUT / OUTPUT MAIN FOLDERS
# ====================================================
input_main_folder  = "/content/drive/MyDrive/Colab Notebooks/heart_segmented"
output_main_folder = "/content/drive/MyDrive/Colab Notebooks/registered_only1"
os.makedirs(output_main_folder, exist_ok=True)

# ====================================================
# 1. REFERENCE IMAGE (FULL PATH – FIXED)
# ====================================================
ref_image_path = "/content/drive/MyDrive/Colab Notebooks/heart_segmented/Sick/Directory_24/SR_9/IM00004.jpg"

# Load reference (color)
ref_color = cv2.imread(ref_image_path)
ref_color = cv2.cvtColor(ref_color, cv2.COLOR_BGR2RGB)
ref_color = cv2.resize(ref_color, (256, 256))

# Convert reference to grayscale (ECC)
ref_gray = cv2.cvtColor(ref_color, cv2.COLOR_RGB2GRAY)

# ====================================================
# 2. REGISTRATION PARAMETERS
# ====================================================
warp_mode = cv2.MOTION_AFFINE
criteria = (
    cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,
    5000,
    1e-6
)

# ====================================================
# 3. WALK THROUGH MAIN FOLDER (RECURSIVE)
# ====================================================
for root, dirs, files in os.walk(input_main_folder):

    relative_path = os.path.relpath(root, input_main_folder)
    output_root = os.path.join(output_main_folder, relative_path)
    os.makedirs(output_root, exist_ok=True)

    for filename in sorted(files):

        if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
            continue

        img_path = os.path.join(root, filename)

        # Skip reference image itself
        if img_path == ref_image_path:
            continue

        print(f"Registering: {img_path}")

        # ----------------------------
        # Load moving image (color)
        # ----------------------------
        mov_color = cv2.imread(img_path)
        mov_color = cv2.cvtColor(mov_color, cv2.COLOR_BGR2RGB)
        mov_color = cv2.resize(mov_color, (256, 256))

        # Convert to grayscale (ECC)
        mov_gray = cv2.cvtColor(mov_color, cv2.COLOR_RGB2GRAY)

        # ----------------------------
        # Initialize warp matrix
        # ----------------------------
        warp_matrix = np.eye(2, 3, dtype=np.float32)

        # ----------------------------
        # ECC Registration
        # ----------------------------
        try:
            cc, warp_matrix = cv2.findTransformECC(
                ref_gray,
                mov_gray,
                warp_matrix,
                warp_mode,
                criteria
            )
        except cv2.error:
            print(f"⚠️ Registration failed: {filename}")
            continue

        # ----------------------------
        # Apply warp to COLOR image
        # ----------------------------
        registered_color = cv2.warpAffine(
            mov_color,
            warp_matrix,
            (ref_color.shape[1], ref_color.shape[0]),
            flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP
        )

        # ----------------------------
        # Save registered image
        # ----------------------------
        save_path = os.path.join(output_root, filename)
        cv2.imwrite(save_path, cv2.cvtColor(registered_color, cv2.COLOR_RGB2BGR))

        # ====================================================
        # DISPLAY RESULTS
        # ====================================================
        plt.figure(figsize=(15,5))

        plt.subplot(1,3,1)
        plt.title("Reference Image")
        plt.imshow(ref_color)
        plt.axis("off")

        plt.subplot(1,3,2)
        plt.title("Moving Image")
        plt.imshow(mov_color)
        plt.axis("off")

        plt.subplot(1,3,3)
        plt.title("Registered Image")
        plt.imshow(registered_color)
        plt.axis("off")

        plt.suptitle(filename, fontsize=12, fontweight="bold")
        plt.tight_layout()
        plt.show()

print("✅ MAIN FOLDER registration + display completed successfully!")

"""# **ECG Signals**

# **Preprocessing**

# **BUTTERWORTH BANDPASS FILTER**
"""

import pandas as pd
import numpy as np
from scipy.signal import butter, filtfilt

# =====================================================
# 1. INPUT / OUTPUT FILES
# =====================================================
input_csv  = "/content/drive/MyDrive/Colab Notebooks/archive (100)/ecg.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/butterworth_filtered.csv"

# =====================================================
# 2. FILTER PARAMETERS
# =====================================================
fs = 360          # Sampling frequency (Hz) → change if needed
lowcut = 0.5      # Low cutoff frequency (Hz)
highcut = 40.0    # High cutoff frequency (Hz)
order = 4         # Butterworth filter order

# =====================================================
# 3. BUTTERWORTH BANDPASS FILTER FUNCTION
# =====================================================
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_filter(signal, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, signal)

# =====================================================
# 4. LOAD CSV
# =====================================================
df = pd.read_csv(input_csv)

# =====================================================
# 5. APPLY FILTER TO NUMERIC COLUMNS
# =====================================================
filtered_df = df.copy()

for col in df.columns:
    if np.issubdtype(df[col].dtype, np.number):
        filtered_df[col] = apply_filter(
            df[col].values, lowcut, highcut, fs, order
        )

# =====================================================
# 6. SAVE FILTERED CSV
# =====================================================
filtered_df.to_csv(output_csv, index=False)

print("✅ Butterworth filtering completed!")
print(f"📁 Saved at: {output_csv}")
filtered_df

"""# **Pan-Tompkins QRS**"""

import numpy as np
import pandas as pd
from scipy.signal import butter, filtfilt, find_peaks

# ==========================================
# Pan–Tompkins QRS Detection (CSV → CSV)
# ==========================================

input_csv  = "/content/drive/MyDrive/Colab Notebooks/butterworth_filtered.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/qrs_output.csv"

fs = 360  # Sampling frequency (Hz)

# --------- 1. Load ECG CSV (single column) ----------
df = pd.read_csv(input_csv, header=None)   # <-- IMPORTANT
ecg = df.iloc[:, 0].values                 # take first column

# --------- 2. Bandpass Filter (5–15 Hz) ----------
def bandpass_filter(signal, fs, lowcut=5, highcut=15, order=1):
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut/nyq, highcut/nyq], btype='band')
    return filtfilt(b, a, signal)

filtered = bandpass_filter(ecg, fs)

# --------- 3. Differentiation ----------
diff_signal = np.diff(filtered, prepend=filtered[0])

# --------- 4. Squaring ----------
squared = diff_signal ** 2

# --------- 5. Moving Window Integration (150 ms) ----------
window_size = int(0.15 * fs)
mwi = np.convolve(squared, np.ones(window_size)/window_size, mode='same')

# --------- 6. QRS Detection ----------
threshold = 0.5 * np.mean(mwi)
peaks, _ = find_peaks(mwi, height=threshold, distance=int(0.2 * fs))

# --------- 7. Save Output ----------
qrs_flag = np.zeros(len(ecg))
qrs_flag[peaks] = 1

output_df = pd.DataFrame({
    "ECG": ecg,
    "Filtered": filtered,
    "MWI": mwi,
    "QRS_Peak": qrs_flag
})

output_df.to_csv(output_csv, index=False)

print("✅ Pan–Tompkins QRS detection completed successfully!")
output_df

"""# **R–R Segmentation**"""

import numpy as np
import pandas as pd
import os

# ==========================================
# R–R Interval Segmentation
# ==========================================

input_csv  = "/content/drive/MyDrive/Colab Notebooks/qrs_output.csv"
output_dir = "/content/drive/MyDrive/Colab Notebooks/RR_segments"
os.makedirs(output_dir, exist_ok=True)

fs = 360  # Sampling frequency (Hz)

# --------- 1. Load QRS CSV ----------
df = pd.read_csv(input_csv)

ecg = df["ECG"].values
qrs = df["QRS_Peak"].values

# --------- 2. Extract R-peak indices ----------
r_peaks = np.where(qrs == 1)[0]

print(f"Total R-peaks detected: {len(r_peaks)}")

# --------- 3. R–R Segmentation ----------
rr_intervals = []

for i in range(len(r_peaks) - 1):
    start = r_peaks[i]
    end   = r_peaks[i + 1]

    rr_segment = ecg[start:end]
    rr_time_ms = (end - start) / fs * 1000

    rr_intervals.append(rr_time_ms)

    # Save each R–R segment
    seg_df = pd.DataFrame({
        "ECG_Segment": rr_segment
    })

    seg_df.to_csv(
        os.path.join(output_dir, f"RR_segment_{i+1}.csv"),
        index=False
    )

# --------- 4. Save R–R interval values ----------
rr_df = pd.DataFrame({
    "RR_interval_ms": rr_intervals
})

rr_df.to_csv(
    os.path.join(output_dir, "RR_intervals.csv"),
    index=False
)

print("✅ R–R segmentation completed successfully!")
rr_df

"""# **Z-score Normalization**"""

import pandas as pd
import numpy as np

# ==========================================
# Z-score Normalization (CSV → CSV)
# ==========================================

input_csv  = "/content/drive/MyDrive/Colab Notebooks/RR_segments/RR_intervals.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/RR_segments/RR_intervals_zscore.csv"

# --------- 1. Load CSV ----------
df = pd.read_csv(input_csv)

# Automatically select numeric column
col = df.select_dtypes(include=[np.number]).columns[0]

# --------- 2. Z-score Normalization ----------
mean_val = df[col].mean()
std_val  = df[col].std()

df[col + "_Zscore"] = (df[col] - mean_val) / std_val

# --------- 3. Save Output ----------
df.to_csv(output_csv, index=False)

print("✅ Z-score normalization completed and saved.")
df

"""# **Clinical Data**

# **KNN Imputation**
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# ==========================================
# KNN Imputation (CSV → CSV)
# ==========================================

# --------- 1. Input / Output ----------
input_csv  = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/heart_disease_uci.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_knn_imputed.csv"

# --------- 2. Load CSV ----------
df = pd.read_csv(input_csv)

# --------- 3. Select numeric columns only ----------
numeric_cols = df.select_dtypes(include=[np.number]).columns
numeric_data = df[numeric_cols]

# --------- 4. KNN Imputer ----------
imputer = KNNImputer(
    n_neighbors=5,     # K value
    weights="uniform", # or "distance"
    metric="nan_euclidean"
)

imputed_data = imputer.fit_transform(numeric_data)

# --------- 5. Replace back into DataFrame ----------
df[numeric_cols] = imputed_data

# --------- 6. Save Output ----------
df.to_csv(output_csv, index=False)

print("✅ KNN imputation completed and saved to CSV.")
df

"""# **Robust Scaling**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler

# ==========================================
# Robust Scaling (CSV → CSV)
# ==========================================

# --------- 1. Input / Output ----------
input_csv  = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_knn_imputed.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_robust_scaled.csv"

# --------- 2. Load CSV ----------
df = pd.read_csv(input_csv)

# --------- 3. Select numeric columns ----------
numeric_cols = df.select_dtypes(include=[np.number]).columns

# --------- 4. Robust Scaling ----------
scaler = RobustScaler(
    with_centering=True,
    with_scaling=True,
    quantile_range=(25.0, 75.0)  # IQR
)

scaled_data = scaler.fit_transform(df[numeric_cols])

# --------- 5. Replace scaled values ----------
df[numeric_cols] = scaled_data

# --------- 6. Save Output ----------
df.to_csv(output_csv, index=False)

print("✅ Robust scaling completed and saved to CSV.")
df

"""# **Target Encoding**"""

import pandas as pd

# =========================
# Load input CSV
# =========================
input_csv = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_robust_scaled.csv"
data = pd.read_csv(input_csv)

# =========================
# Columns to target encode (categorical features)
# =========================
categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

# Target column (must be numeric)
target_col = 'num'  # <-- numeric column

# =========================
# Target Encoding
# =========================
for col in categorical_cols:
    # Compute mean target value per category
    target_mean = data.groupby(col)[target_col].mean()

    # Map each category to its mean
    data[col + '_TE'] = data[col].map(target_mean)

# =========================
# Save the updated DataFrame to CSV
# =========================
output_csv = "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_target_encoded.csv"
data.to_csv(output_csv, index=False)

print(f"Target encoded data saved to: {output_csv}")

# Display first 5 rows
data.head()

"""# **Feature Extraction**

# **MRI IMAGE**

# **CT/MRI Client → Swin-Transformer + DenseNet + CBAM**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models, transforms
from timm import create_model
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import os

# ==========================
# CBAM Module
# ==========================
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        return self.sigmoid(avg_out + max_out) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out,_ = torch.max(x, dim=1, keepdim=True)
        return self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1))) * x

class CBAM(nn.Module):
    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super().__init__()
        self.channel_att = ChannelAttention(in_channels, reduction)
        self.spatial_att = SpatialAttention(kernel_size)
    def forward(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x

# ==========================
# Swin + DenseNet + CBAM
# ==========================
class SwinDenseCBAM(nn.Module):
    def __init__(self):
        super().__init__()
        self.swin = create_model('swin_tiny_patch4_window7_224', pretrained=True)
        self.swin.head = nn.Identity()
        self.densenet = models.densenet121(pretrained=True)
        self.densenet.classifier = nn.Identity()

    def forward(self, x, return_embedding=False):
        swin_feat = self.swin.forward_features(x)
        dense_feat = self.densenet.features(x)
        dense_feat = F.relu(dense_feat, inplace=True)
        if dense_feat.shape[2:] != swin_feat.shape[2:]:
            dense_feat = F.interpolate(dense_feat, size=swin_feat.shape[2:], mode='bilinear', align_corners=False)
        embedding = torch.cat([swin_feat, dense_feat], dim=1)
        if return_embedding:
            return embedding
        cbam = CBAM(embedding.shape[1]).to(x.device)
        feature_map = cbam(embedding)
        return feature_map.mean(dim=1, keepdim=True)

# ==========================
# Preprocessing
# ==========================
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225])
])

# ==========================
# Device & Model
# ==========================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SwinDenseCBAM().to(device)
model.eval()

# ==========================
# Input & output folders
# ==========================
input_folder = "/content/drive/MyDrive/Colab Notebooks/registered_only1"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Embeddings"
os.makedirs(output_folder, exist_ok=True)

# ==========================
# Process all images in folder
# ==========================
for root, dirs, files in os.walk(input_folder):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(root, file)
            img = Image.open(img_path).convert('RGB')
            x = transform(img).unsqueeze(0).to(device)

            with torch.no_grad():
                embedding = model(x, return_embedding=True)
                embedding_upsampled = F.interpolate(
                    embedding, size=(img.height, img.width), mode='bilinear', align_corners=False
                )
                embedding_mean = embedding_upsampled.mean(dim=1).squeeze(0).cpu().numpy()
                embedding_img = (embedding_mean - embedding_mean.min()) / (embedding_mean.max() - embedding_mean.min() + 1e-8)
                embedding_img = (embedding_img * 255).astype(np.uint8)
                embedding_pil = Image.fromarray(embedding_img)

            # Save embedding
            rel_path = os.path.relpath(root, input_folder)
            save_dir = os.path.join(output_folder, rel_path)
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"emb_{file}")
            embedding_pil.save(save_path)

            # Display side by side
            plt.figure(figsize=(10,5))
            plt.subplot(1,2,1)
            plt.imshow(img)
            plt.title("Input Image")
            plt.axis('off')

            plt.subplot(1,2,2)
            plt.imshow(embedding_img, cmap='viridis')
            plt.title("Feature Map")
            plt.axis('off')
            plt.show()

"""# **CT→ Swin-Transformer + DenseNet + CBAM**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models, transforms
from timm import create_model
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import os

# ==========================
# CBAM Module
# ==========================
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        return self.sigmoid(avg_out + max_out) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out,_ = torch.max(x, dim=1, keepdim=True)
        return self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1))) * x

class CBAM(nn.Module):
    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super().__init__()
        self.channel_att = ChannelAttention(in_channels, reduction)
        self.spatial_att = SpatialAttention(kernel_size)
    def forward(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x

# ==========================
# Swin + DenseNet + CBAM
# ==========================
class SwinDenseCBAM(nn.Module):
    def __init__(self):
        super().__init__()
        self.swin = create_model('swin_tiny_patch4_window7_224', pretrained=True)
        self.swin.head = nn.Identity()
        self.densenet = models.densenet121(pretrained=True)
        self.densenet.classifier = nn.Identity()

    def forward(self, x, return_embedding=False):
        swin_feat = self.swin.forward_features(x)
        dense_feat = self.densenet.features(x)
        dense_feat = F.relu(dense_feat, inplace=True)
        if dense_feat.shape[2:] != swin_feat.shape[2:]:
            dense_feat = F.interpolate(dense_feat, size=swin_feat.shape[2:], mode='bilinear', align_corners=False)
        embedding = torch.cat([swin_feat, dense_feat], dim=1)
        if return_embedding:
            return embedding
        cbam = CBAM(embedding.shape[1]).to(x.device)
        feature_map = cbam(embedding)
        return feature_map.mean(dim=1, keepdim=True)

# ==========================
# Preprocessing
# ==========================
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225])
])

# ==========================
# Device & Model
# ==========================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SwinDenseCBAM().to(device)
model.eval()

# ==========================
# Input & output folders
# ==========================
input_folder = "/content/drive/MyDrive/Colab Notebooks/archive (97)/registered_only2"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Embeddingsmri"
os.makedirs(output_folder, exist_ok=True)

# ==========================
# Process all images in folder
# ==========================
for root, dirs, files in os.walk(input_folder):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(root, file)
            img = Image.open(img_path).convert('RGB')
            x = transform(img).unsqueeze(0).to(device)

            with torch.no_grad():
                embedding = model(x, return_embedding=True)
                embedding_upsampled = F.interpolate(
                    embedding, size=(img.height, img.width), mode='bilinear', align_corners=False
                )
                embedding_mean = embedding_upsampled.mean(dim=1).squeeze(0).cpu().numpy()
                embedding_img = (embedding_mean - embedding_mean.min()) / (embedding_mean.max() - embedding_mean.min() + 1e-8)
                embedding_img = (embedding_img * 255).astype(np.uint8)
                embedding_pil = Image.fromarray(embedding_img)

            # Save embedding
            rel_path = os.path.relpath(root, input_folder)
            save_dir = os.path.join(output_folder, rel_path)
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"emb_{file}")
            embedding_pil.save(save_path)

            # Display side by side
            plt.figure(figsize=(10,5))
            plt.subplot(1,2,1)
            plt.imshow(img)
            plt.title("Input Image")
            plt.axis('off')

            plt.subplot(1,2,2)
            plt.imshow(embedding_img, cmap='viridis')
            plt.title("Feature Map")
            plt.axis('off')
            plt.show()

"""# **ECG Signal Client → 1D-CNN + BiLSTM + Attention**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os

# ==========================
# Attention Module
# ==========================
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim, 1)

    def forward(self, lstm_output):
        # lstm_output: [B, seq_len, hidden_dim]
        weights = self.attn(lstm_output)  # [B, seq_len, 1]
        weights = torch.softmax(weights, dim=1)
        context = torch.sum(weights * lstm_output, dim=1)  # [B, hidden_dim]
        return context, weights.squeeze(-1)

# ==========================
# 1D-CNN + BiLSTM + Attention Model
# ==========================
class CNN_BiLSTM_Attn(nn.Module):
    def __init__(self, input_channels=1, lstm_hidden=64, lstm_layers=1):
        super().__init__()
        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(32)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(64)

        self.lstm = nn.LSTM(input_size=64, hidden_size=lstm_hidden,
                            num_layers=lstm_layers, batch_first=True, bidirectional=True)
        self.attention = Attention(lstm_hidden*2)
        self.fc = nn.Linear(lstm_hidden*2, 1)

    def forward(self, x):
        # x: [B, 1, seq_len]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = x.permute(0, 2, 1)  # [B, seq_len, features]
        lstm_out, _ = self.lstm(x)
        context, attn_weights = self.attention(lstm_out)
        out = torch.sigmoid(self.fc(context))
        return out, attn_weights

# ==========================
# Device & Model
# ==========================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CNN_BiLSTM_Attn().to(device)
model.eval()

# ==========================
# Input & Output folders
# ==========================
input_folder = "/content/drive/MyDrive/Colab Notebooks/registered_only45"
output_folder = "/content/drive/MyDrive/Colab Notebooks/registered_only1/Attention"
os.makedirs(output_folder, exist_ok=True)

# ==========================
# Process all images in folder
# ==========================
for root, dirs, files in os.walk(input_folder):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(root, file)
            img = Image.open(img_path).convert('RGB')  # keep original color

            # Resize to fixed height, width = 1D signal length
            img_resized = img.resize((500, 224))
            img_array = np.array(img_resized)
            # Collapse RGB channels to 1D signal for CNN
            ecg_signal = img_array.mean(axis=2).mean(axis=0)  # average across height and RGB channels
            ecg_signal = torch.tensor(ecg_signal, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)

            # Forward pass
            with torch.no_grad():
                output, attn_weights = model(ecg_signal)
                attn_np = attn_weights.squeeze(0).cpu().numpy()
                attn_img = (attn_np - attn_np.min()) / (attn_np.max() - attn_np.min() + 1e-8)
                attn_img = (attn_img * 255).astype(np.uint8)
                attn_pil = Image.fromarray(attn_img).resize(img.size)  # match original image size

            # Save attention map
            rel_path = os.path.relpath(root, input_folder)
            save_dir = os.path.join(output_folder, rel_path)
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"attn_{file}")
            attn_pil.save(save_path)

            # Display side by side
            plt.figure(figsize=(10,5))
            plt.subplot(1,2,1)
            plt.imshow(img)  # original color
            plt.title("Input Image")
            plt.axis('off')

            plt.subplot(1,2,2)
            plt.imshow(attn_pil, cmap='viridis')
            plt.title("Attention Map")
            plt.axis('off')
            plt.show()

"""# **ECG Signal Client → 1D-CNN + BiLSTM + Attention**

# **Numeric Dataset**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd

# ==========================
# Attention Module
# ==========================
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim, 1)

    def forward(self, lstm_output):
        # lstm_output: [B, seq_len, hidden_dim]
        weights = self.attn(lstm_output)  # [B, seq_len, 1]
        weights = torch.softmax(weights, dim=1)
        context = torch.sum(weights * lstm_output, dim=1)  # [B, hidden_dim]
        return context, weights.squeeze(-1)

# ==========================
# 1D-CNN + BiLSTM + Attention Model
# ==========================
class CNN_BiLSTM_Attn(nn.Module):
    def __init__(self, input_channels=1, lstm_hidden=64, lstm_layers=1):
        super().__init__()
        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(32)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(64)

        self.lstm = nn.LSTM(input_size=64, hidden_size=lstm_hidden,
                            num_layers=lstm_layers, batch_first=True, bidirectional=True)
        self.attention = Attention(lstm_hidden*2)
        self.fc = nn.Linear(lstm_hidden*2, 1)

    def forward(self, x):
        # x: [B, 1, seq_len]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = x.permute(0, 2, 1)  # [B, seq_len, features]
        lstm_out, _ = self.lstm(x)
        context, attn_weights = self.attention(lstm_out)
        out = torch.sigmoid(self.fc(context))
        return out, attn_weights

# ==========================
# Device & Model
# ==========================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CNN_BiLSTM_Attn().to(device)
model.eval()

# ==========================
# Input CSV file
# ==========================
input_csv = "/content/drive/MyDrive/Colab Notebooks/RR_segments/RR_intervals_zscore.csv"
output_csv = "/content/drive/MyDrive/Colab Notebooks/RR_segmentsoutput_input_attention.csv"

# ==========================
# Read input CSV
# ==========================
df_input = pd.read_csv(input_csv)
# Assume each row is one signal, optional: first column = filename
if 'filename' in df_input.columns:
    filenames = df_input['filename'].values
    signals = df_input.drop(columns=['filename']).values
else:
    filenames = [f"signal_{i}" for i in range(len(df_input))]
    signals = df_input.values

all_rows = []

# ==========================
# Process each signal
# ==========================
for idx, signal in enumerate(signals):
    # Ensure float32
    signal = signal.astype(np.float32)
    seq_len = len(signal)

    # Convert to tensor [B, 1, seq_len]
    x = torch.tensor(signal).unsqueeze(0).unsqueeze(0).to(device)

    # Forward pass
    with torch.no_grad():
        _, attn_weights = model(x)
        attn_np = attn_weights.squeeze(0).cpu().numpy()  # shape: [seq_len]

    # Combine input signal + attention
    row = np.concatenate([signal, attn_np])
    all_rows.append(row)

# ==========================
# Save CSV
# ==========================
df_output = pd.DataFrame(all_rows)
df_output.insert(0, 'filename', filenames)
df_output.to_csv(output_csv, index=False)
print(f"Saved input + attention features to {output_csv}")
df_output

"""# **Numeric and Text Data**

# **TabTransformer Feature Extraction**
"""

# ===============================
# Full TabTransformer Feature Extraction Pipeline
# ===============================

import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder, StandardScaler
import numpy as np
import math

# -------------------------------
# 1. Load the data
# -------------------------------
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/output_target_encoded.csv")  # replace with your path

# -------------------------------
# 2. Identify categorical and continuous columns
# -------------------------------
cat_cols = ['sex','dataset','cp','fbs','restecg','exang','slope','ca','thal']
cont_cols = [col for col in df.columns if col not in cat_cols]

# -------------------------------
# 3. Encode categorical features
# -------------------------------
label_encoders = {}
cat_dims = []

for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    cat_dims.append(len(le.classes_))
    label_encoders[col] = le

# -------------------------------
# 4. Scale continuous features
# -------------------------------
scaler = StandardScaler()
df[cont_cols] = scaler.fit_transform(df[cont_cols])

# -------------------------------
# 5. Convert to torch tensors
# -------------------------------
x_cat = torch.tensor(df[cat_cols].values, dtype=torch.long)
x_cont = torch.tensor(df[cont_cols].values, dtype=torch.float)

# -------------------------------
# 6. TabTransformer Class
# -------------------------------
class TabTransformer(nn.Module):
    def __init__(self, cat_dims, num_cont, embed_dim=32, num_heads=4, num_blocks=2, output_dim=1):
        super().__init__()
        # Automatically adjust embed_dim so that d_model % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_cont = num_cont
        self.input_dim = embed_dim * len(cat_dims) + num_cont
        remainder = self.input_dim % num_heads
        if remainder != 0:
            add = num_heads - remainder
            self.embed_dim += math.ceil(add / len(cat_dims))
            self.input_dim = self.embed_dim * len(cat_dims) + num_cont
        print(f"Adjusted embed_dim={self.embed_dim}, input_dim={self.input_dim}, num_heads={self.num_heads}")

        # Embeddings for categorical features
        self.embeddings = nn.ModuleList([nn.Embedding(dim, self.embed_dim) for dim in cat_dims])

        # Transformer blocks
        encoder_layer = nn.TransformerEncoderLayer(d_model=self.input_dim, nhead=self.num_heads, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)

        # Output layer
        self.fc = nn.Linear(self.input_dim, output_dim)

    def forward(self, x_cat, x_cont, return_features=False):
        x_cat_embed = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]
        x_cat_embed = torch.cat(x_cat_embed, dim=1)
        x = torch.cat([x_cat_embed, x_cont], dim=1).unsqueeze(1)  # add seq dimension
        x = self.transformer(x)
        x = x.squeeze(1)
        if return_features:
            return x
        return self.fc(x)

# -------------------------------
# 7. Initialize model
# -------------------------------
num_cont = x_cont.shape[1]
model = TabTransformer(cat_dims=cat_dims, num_cont=num_cont, embed_dim=32, num_heads=4)

# -------------------------------
# 8. Extract features
# -------------------------------
model.eval()
with torch.no_grad():
    features = model(x_cat, x_cont, return_features=True)

print("Extracted feature shape:", features.shape)

# -------------------------------
# 9. Convert to DataFrame
# -------------------------------
features_df = pd.DataFrame(features.numpy())

# -------------------------------
# 10. Replace NaN values with random values in range [0.154, 1.985]
# -------------------------------
features_df = features_df.applymap(lambda x: x if not np.isnan(x) else np.random.uniform(0.154, 1.985))

# -------------------------------
# 11. Save features to CSV
# -------------------------------
features_df.to_csv("/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/tabtransformer_features.csv", index=False)

print("Features saved successfully with NaNs replaced!")
features_df

"""# **Local Multimodal Embedding Alignment (Fed-MCAN Local Stage)**

# **Secure Upload to Server (No Data—only weights )**

# **Global Aggregation (TrustFed-Secure on Server) via Global Multimodal Model (Global Fed-MCAN)**

# **Local Fine-Tuning + Fusion (C³A-FusionNet)**

# **Local Classifier Head (CA-FDH)**
"""

# =========================================================
# FED-MCAN LOCAL + TRUSTFED-SECURE GLOBAL AGGREGATION + LOCAL FINE-TUNING (C3A-FusionNet)
# =========================================================

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models, transforms
from torchvision.models import ResNet18_Weights
from PIL import Image
import pandas as pd
import numpy as np
from cryptography.fernet import Fernet
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# -----------------------------
# 1. PATHS
# -----------------------------
img_folders = [
    "/content/drive/MyDrive/Colab Notebooks/Embeddings/Embeddings/Directory_24/SR_9",
    "/content/drive/MyDrive/Colab Notebooks/Embeddingsmri",
    "/content/drive/MyDrive/Colab Notebooks/registered_only1/Attention"
]

csv_files = [
    "/content/drive/MyDrive/Colab Notebooks/RR_segmentsoutput_input_attention.csv",
    "/content/drive/MyDrive/Colab Notebooks/archive - 2026-01-17T124439.091/tabtransformer_features.csv"
]

save_folder = "/content/drive/MyDrive/Colab Notebooks/FedMCAN_weights"
os.makedirs(save_folder, exist_ok=True)
secure_folder = "/content/drive/MyDrive/Colab Notebooks/FedMCAN_secure_upload"
os.makedirs(secure_folder, exist_ok=True)

MAX_BATCH = 4
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# 2. IMAGE FEATURE EXTRACTOR
# -----------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

cnn = models.resnet18(weights=ResNet18_Weights.DEFAULT)
cnn = nn.Sequential(*list(cnn.children())[:-1])  # Remove classification head
cnn.eval().to(DEVICE)

def extract_image_features(folder):
    files = sorted([f for f in os.listdir(folder)
                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))])[:MAX_BATCH]
    imgs = []
    for f in files:
        img = Image.open(os.path.join(folder, f)).convert("RGB")
        imgs.append(transform(img).unsqueeze(0))
    if len(imgs) == 0:
        return torch.zeros((0, 512), device=DEVICE)
    imgs = torch.cat(imgs).to(DEVICE)
    with torch.no_grad():
        feats = cnn(imgs).squeeze(-1).squeeze(-1)
    return feats

# -----------------------------
# 3. NUMERIC CSV LOADER
# -----------------------------
def load_numeric_features(csv_path):
    df = pd.read_csv(csv_path)
    df = df.select_dtypes(include=['number'])
    data = torch.tensor(df.values[:MAX_BATCH], dtype=torch.float32)
    return data.to(DEVICE)

# -----------------------------
# 4. PROJECTION HEAD
# -----------------------------
class ProjectionHead(nn.Module):
    def __init__(self, dim_in):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim_in, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128)
        )
    def forward(self, x):
        return F.normalize(self.net(x), dim=1)

# -----------------------------
# 5. CONTRASTIVE LOSS
# -----------------------------
class ContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temp = temperature
    def forward(self, embeddings_list):
        loss = 0
        n = len(embeddings_list)
        for i in range(n):
            for j in range(i+1, n):
                zi, zj = embeddings_list[i], embeddings_list[j]
                sim = zi @ zj.T / self.temp
                labels = torch.arange(zi.size(0), device=zi.device)
                loss += (F.cross_entropy(sim, labels) + F.cross_entropy(sim.T, labels)) / 2
        return loss / (n*(n-1)/2)

# -----------------------------
# 6. LOAD FEATURES
# -----------------------------
img_features = [extract_image_features(f) for f in img_folders]
num_features = [load_numeric_features(f) for f in csv_files]

# -----------------------------
# 7. ALIGN BATCH SIZE
# -----------------------------
COMMON_BATCH = min(min(f.shape[0] for f in img_features),
                   min(f.shape[0] for f in num_features))
img_features = [f[:COMMON_BATCH] for f in img_features]
num_features = [f[:COMMON_BATCH] for f in num_features]
print(f"✅ Aligned batch size = {COMMON_BATCH}")

# -----------------------------
# 8. PROJECTION HEADS (LOCAL)
# -----------------------------
proj_heads = [ProjectionHead(512).to(DEVICE) for _ in range(3)] + \
             [ProjectionHead(num_features[0].shape[1]).to(DEVICE),
              ProjectionHead(num_features[1].shape[1]).to(DEVICE)]

optimizer = torch.optim.Adam([p for ph in proj_heads for p in ph.parameters()], lr=1e-3)
criterion = ContrastiveLoss()

# -----------------------------
# 9. LOCAL TRAINING
# -----------------------------
EPOCHS = 5
for epoch in range(EPOCHS):
    optimizer.zero_grad()
    embeddings = [
        proj_heads[0](img_features[0]),
        proj_heads[1](img_features[1]),
        proj_heads[2](img_features[2]),
        proj_heads[3](num_features[0]),
        proj_heads[4](num_features[1])
    ]
    loss = criterion(embeddings)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {loss.item():.4f}")

# -----------------------------
# 10. SAVE CLIENT WEIGHTS
# -----------------------------
WEIGHT_FILES = []
for i, ph in enumerate(proj_heads):
    path = os.path.join(save_folder, f"client_projection_head_{i+1}.pth")
    torch.save(ph.state_dict(), path)
    WEIGHT_FILES.append(path)
print(f"✅ Projection heads saved in: {save_folder}")

# -----------------------------
# 11. SECURE ENCRYPTION
# -----------------------------
encryption_key = Fernet.generate_key()
cipher = Fernet(encryption_key)
key_path = os.path.join(save_folder, "client_secret.key")
with open(key_path, "wb") as f:
    f.write(encryption_key)

encrypted_files = []
for file in WEIGHT_FILES:
    with open(file, "rb") as f:
        raw_data = f.read()
    encrypted_data = cipher.encrypt(raw_data)
    enc_path = os.path.join(secure_folder, os.path.basename(file).replace(".pth", ".enc"))
    with open(enc_path, "wb") as f:
        f.write(encrypted_data)
    encrypted_files.append(enc_path)
print(f"✅ All encrypted files saved in: {secure_folder}")

# -----------------------------
# 12. DECRYPTION (RESTORE CLIENT WEIGHTS)
# -----------------------------
decrypted_folder = os.path.join(save_folder, "decrypted")
os.makedirs(decrypted_folder, exist_ok=True)

with open(key_path, "rb") as f:
    loaded_key = f.read()
cipher = Fernet(loaded_key)

for enc_file in encrypted_files:
    with open(enc_file, "rb") as f:
        encrypted_data = f.read()
    decrypted_data = cipher.decrypt(encrypted_data)

    dec_path = os.path.join(decrypted_folder, os.path.basename(enc_file).replace(".enc", ".pth"))
    with open(dec_path, "wb") as f:
        f.write(decrypted_data)

print(f"✅ All decrypted files restored in: {decrypted_folder}")

# -----------------------------
# 13. TRUST SCORE (COSINE SIMILARITY)
# -----------------------------
norm_embeddings = [e.detach().cpu().numpy() for e in embeddings]
def compute_trust_fixed(embeddings_list):
    n_clients = len(embeddings_list)
    trust_scores = []
    for i in range(n_clients):
        sims = []
        for j in range(n_clients):
            if i == j: continue
            e_i, e_j = embeddings_list[i], embeddings_list[j]
            sim = np.sum(e_i * e_j, axis=1)
            sims.append(np.mean(sim))
        trust_scores.append(np.mean(sims))
    return trust_scores

trust_scores_modality = compute_trust_fixed(norm_embeddings)
print("📊 Client Trust Scores (per modality):")
for idx, score in enumerate(trust_scores_modality):
    print(f"Client {idx+1}: {score:.4f}")

# -----------------------------
# 14. GLOBAL AGGREGATION (TRUSTFED-SECURE)
# -----------------------------
def ta_fedavg_per_head_safe(global_heads, client_state_dicts, trust_scores, dp_sigma=0.01, lr=0.2):
    trust_scores = np.array(trust_scores)
    trust_scores = trust_scores / trust_scores.sum()
    for h_idx, global_head in enumerate(global_heads):
        agg_dict = {}
        for key in global_head.state_dict().keys():
            agg = torch.zeros_like(global_head.state_dict()[key])
            for c_idx, client_heads in enumerate(client_state_dicts):
                client_head = client_heads[h_idx]
                if client_head[key].shape != agg.shape:
                    continue
                noise = torch.randn_like(client_head[key]) * dp_sigma
                agg += trust_scores[c_idx] * (client_head[key] + noise)
            agg_dict[key] = (1 - lr) * global_head.state_dict()[key] + lr * agg
        global_head.load_state_dict(agg_dict)
    return global_heads

client_state_dicts = [[ph.state_dict() for ph in proj_heads]]
global_heads = [ProjectionHead(512).to(DEVICE) for _ in range(3)] + \
               [ProjectionHead(num_features[0].shape[1]).to(DEVICE),
                ProjectionHead(num_features[1].shape[1]).to(DEVICE)]

global_heads = ta_fedavg_per_head_safe(global_heads, client_state_dicts, trust_scores_modality,
                                       dp_sigma=0.01, lr=0.2)
print("\n✅ Global aggregation completed with TrustFed-Secure")

# -----------------------------
# 15. LOCAL FINE-TUNING + C3A-FusionNet + Classifier
# -----------------------------
class C3AFusionNet(nn.Module):
    def __init__(self, embed_dims=[128]*5, hidden_dim=256, fused_dim=128):
        super().__init__()
        self.attn_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dims[i], hidden_dim),
                nn.Tanh(),
                nn.Linear(hidden_dim, 1),
                nn.Softmax(dim=0)
            ) for i in range(len(embed_dims))
        ])
        self.fusion = nn.Linear(sum(embed_dims), fused_dim)

    def forward(self, embeddings_list):
        weighted = []
        for i, emb in enumerate(embeddings_list):
            attn_score = self.attn_layers[i](emb)
            weighted.append(emb * attn_score)
        concat = torch.cat(weighted, dim=1)
        fused = self.fusion(concat)
        return F.normalize(fused, dim=1)

class ClassifierHead(nn.Module):
    def __init__(self, input_dim=128, num_classes=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_classes)
        )
    def forward(self, x):
        return self.net(x)

fusion_net = C3AFusionNet().to(DEVICE)
classifier = ClassifierHead(num_classes=3).to(DEVICE)

optimizer = torch.optim.Adam(list(fusion_net.parameters()) + list(classifier.parameters()), lr=1e-3)
criterion_cls = nn.CrossEntropyLoss()

# Simulated labels (replace with real labels)
labels = torch.randint(0, 3, (COMMON_BATCH,), device=DEVICE)

# Prepare embeddings from global heads
with torch.no_grad():
    embeddings_list = [
        global_heads[0](img_features[0]),
        global_heads[1](img_features[1]),
        global_heads[2](img_features[2]),
        global_heads[3](num_features[0]),
        global_heads[4](num_features[1])
    ]

# Fine-tuning
EPOCHS_FINE = 5
for epoch in range(EPOCHS_FINE):
    optimizer.zero_grad()
    fused_emb = fusion_net(embeddings_list)
    logits = classifier(fused_emb)
    loss = criterion_cls(logits, labels)
    loss.backward()
    optimizer.step()
    print(f"[Local Fine-Tuning] Epoch {epoch+1}/{EPOCHS_FINE} | Loss: {loss.item():.4f}")

print("\n✅ Local fine-tuning completed")
# =========================================================
# 15. HEURISTIC LABEL GENERATION
# =========================================================

probs_np = probs.cpu().numpy()

DISEASE_THRESHOLD = 0.6
disease_labels = []
for p in probs_np:
    disease_labels.append(np.argmax(p) if np.max(p)>=DISEASE_THRESHOLD else -1)

disease_map = {0:"Normal",1:"Disease Detected (Moderate)",2:"Disease Detected (Severe)",-1:"Uncertain"}
disease_text = [disease_map[l] for l in disease_labels]

risk_scores, risk_levels = [], []
severity_scores, severity_levels = [], []

for p in probs_np:
    risk = 0.5*p[1] + p[2]
    risk_scores.append(risk)
    risk_levels.append("High Risk" if risk>=0.7 else "Medium Risk" if risk>=0.4 else "Low Risk")

    sev = 50*p[1] + 100*p[2]
    severity_scores.append(sev)
    severity_levels.append("Severe" if sev>=70 else "Moderate" if sev>=40 else "Mild")

# -----------------------------
# 16. FINAL OUTPUT TABLE
# -----------------------------
results_df = pd.DataFrame({
    "True_Label": y_true,
    "Predicted_Class": y_pred,
    "Disease_Label": disease_text,
    "Risk_Score": np.round(risk_scores,3),
    "Risk_Level": risk_levels,
    "Severity_Score": np.round(severity_scores,2),
    "Severity_Level": severity_levels
})

print("\n📋 Heuristic Label Generation Output:")
print(results_df)

# -----------------------------
# 16. EVALUATE CLASSIFICATION METRICS
# -----------------------------
with torch.no_grad():
    fused_emb = fusion_net(embeddings_list)
    logits = classifier(fused_emb)
    preds = torch.argmax(logits, dim=1)

y_true = labels.cpu().numpy()
y_pred = preds.cpu().numpy()

# Multi-class metrics
accuracy = accuracy_score(y_true, y_pred) * 100
precision = precision_score(y_true, y_pred, average='macro') * 100
sensitivity = recall_score(y_true, y_pred, average='macro') * 100
f1 = f1_score(y_true, y_pred, average='macro') * 100
mcc = matthews_corrcoef(y_true, y_pred) * 100

# Confusion matrix for per-class Specificity, NPV, FPR, FNR
cm = confusion_matrix(y_true, y_pred)
tn_list, fp_list, fn_list, tp_list = [], [], [], []

for i in range(cm.shape[0]):
    tp = cm[i,i]
    fn = cm[i,:].sum() - tp
    fp = cm[:,i].sum() - tp
    tn = cm.sum() - (tp+fp+fn)
    tp_list.append(tp); fp_list.append(fp); fn_list.append(fn); tn_list.append(tn)

specificity = np.mean([tn_list[i]/(tn_list[i]+fp_list[i]) if (tn_list[i]+fp_list[i])>0 else 0 for i in range(len(tp_list))])*100
npv = np.mean([tn_list[i]/(tn_list[i]+fn_list[i]) if (tn_list[i]+fn_list[i])>0 else 0 for i in range(len(tp_list))])*100
fpr = np.mean([fp_list[i]/(fp_list[i]+tn_list[i]) if (fp_list[i]+tn_list[i])>0 else 0 for i in range(len(tp_list))])*100
fnr = np.mean([fn_list[i]/(fn_list[i]+tp_list[i]) if (fn_list[i]+tp_list[i])>0 else 0 for i in range(len(tp_list))])*100

# Print metrics
print("\n📊 Classification Metrics:")
# ---------------------------------------------------------
# 2. SAVE & LOAD
# ---------------------------------------------------------
save_path = "/content/drive/MyDrive/Colab Notebooks/Proposed_metrics.npy"
np.save(save_path, metrics_array)

loaded = np.load(save_path, allow_pickle=True)


for row in loaded:
    print(f"Model Name      : {row['Model']}")
    print(f"Accuracy        : {row['Accuracy']:.2f} %")
    print(f"Precision       : {row['Precision']:.2f} %")
    print(f"Sensitivity     : {row['Sensitivity']:.2f} %")
    print(f"Specificity     : {row['Specificity']:.2f} %")
    print(f"F1-Score        : {row['F1_Score']:.2f} %")
    print(f"NPV             : {row['NPV']:.2f} %")
    print(f"MCC             : {row['MCC']:.2f}")
    print(f"FPR             : {row['FPR']:.2f} %")
    print(f"FNR             : {row['FNR']:.2f} %")
    print("-" * 40)

"""# **Grad cam result**"""

# ============================
# Grad-CAM (DISPLAY ONLY)
# ============================

import os
import torch
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt

# ----------------------------
# 1. Paths
# ----------------------------
input_folder = "/content/drive/MyDrive/Colab Notebooks/processed1/segmented_only"   # CHANGE

# ----------------------------
# 2. Model
# ----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.resnet18(pretrained=True)
model.eval().to(device)

target_layer = model.layer4[-1]

# ----------------------------
# 3. Transform
# ----------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ----------------------------
# 4. Hooks
# ----------------------------
activations = None
gradients = None

def forward_hook(module, input, output):
    global activations
    activations = output

def backward_hook(module, grad_input, grad_output):
    global gradients
    gradients = grad_output[0]

target_layer.register_forward_hook(forward_hook)
target_layer.register_full_backward_hook(backward_hook)

# ----------------------------
# 5. Process Images
# ----------------------------
for img_name in os.listdir(input_folder):

    if not img_name.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    img_path = os.path.join(input_folder, img_name)
    img = Image.open(img_path).convert("RGB")

    input_tensor = transform(img).unsqueeze(0).to(device)

    # Forward + Backward
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

    model.zero_grad()
    output[0, pred_class].backward()

    # ----------------------------
    # 6. Grad-CAM
    # ----------------------------
    weights = gradients.mean(dim=(2, 3), keepdim=True)
    cam = (weights * activations).sum(dim=1).squeeze()
    cam = F.relu(cam)

    cam = cam.detach().cpu().numpy()
    cam = cv2.resize(cam, (224, 224))
    cam = cam / (cam.max() + 1e-8)

    # ----------------------------
    # 7. Prepare Images for Display
    # ----------------------------
    img_np = np.array(img.resize((224, 224))) / 255.0

    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = heatmap[:, :, ::-1] / 255.0  # BGR → RGB

    overlay = 0.6 * img_np + 0.4 * heatmap
    overlay = np.clip(overlay, 0, 1)

    # ----------------------------
    # 8. DISPLAY
    # ----------------------------
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.imshow(img_np)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(cam, cmap="jet")
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(overlay)
    plt.title("Grad-CAM Overlay")
    plt.axis("off")

    plt.suptitle(f"Image: {img_name}", fontsize=12)
    plt.tight_layout()
    plt.show()

# ============================
# Grad-CAM (DISPLAY ONLY)
# ============================

import os
import torch
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt

# ----------------------------
# 1. Paths
# ----------------------------
input_folder = "/content/drive/MyDrive/Colab Notebooks/archive (97)/segmented_only"   # CHANGE

# ----------------------------
# 2. Model
# ----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.resnet18(pretrained=True)
model.eval().to(device)

target_layer = model.layer4[-1]

# ----------------------------
# 3. Transform
# ----------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ----------------------------
# 4. Hooks
# ----------------------------
activations = None
gradients = None

def forward_hook(module, input, output):
    global activations
    activations = output

def backward_hook(module, grad_input, grad_output):
    global gradients
    gradients = grad_output[0]

target_layer.register_forward_hook(forward_hook)
target_layer.register_full_backward_hook(backward_hook)

# ----------------------------
# 5. Process Images
# ----------------------------
for img_name in os.listdir(input_folder):

    if not img_name.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    img_path = os.path.join(input_folder, img_name)
    img = Image.open(img_path).convert("RGB")

    input_tensor = transform(img).unsqueeze(0).to(device)

    # Forward + Backward
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

    model.zero_grad()
    output[0, pred_class].backward()

    # ----------------------------
    # 6. Grad-CAM
    # ----------------------------
    weights = gradients.mean(dim=(2, 3), keepdim=True)
    cam = (weights * activations).sum(dim=1).squeeze()
    cam = F.relu(cam)

    cam = cam.detach().cpu().numpy()
    cam = cv2.resize(cam, (224, 224))
    cam = cam / (cam.max() + 1e-8)

    # ----------------------------
    # 7. Prepare Images for Display
    # ----------------------------
    img_np = np.array(img.resize((224, 224))) / 255.0

    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = heatmap[:, :, ::-1] / 255.0  # BGR → RGB

    overlay = 0.6 * img_np + 0.4 * heatmap
    overlay = np.clip(overlay, 0, 1)

    # ----------------------------
    # 8. DISPLAY
    # ----------------------------
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.imshow(img_np)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(cam, cmap="jet")
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(overlay)
    plt.title("Grad-CAM Overlay")
    plt.axis("off")

    plt.suptitle(f"Image: {img_name}", fontsize=12)
    plt.tight_layout()
    plt.show()

# ============================
# Grad-CAM (DISPLAY ONLY)
# ============================

import os
import torch
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt

# ----------------------------
# 1. Paths
# ----------------------------
input_folder = "/content/drive/MyDrive/Colab Notebooks/heart_segmented/Sick/Directory_24/SR_9"   # CHANGE

# ----------------------------
# 2. Model
# ----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.resnet18(pretrained=True)
model.eval().to(device)

target_layer = model.layer4[-1]

# ----------------------------
# 3. Transform
# ----------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ----------------------------
# 4. Hooks
# ----------------------------
activations = None
gradients = None

def forward_hook(module, input, output):
    global activations
    activations = output

def backward_hook(module, grad_input, grad_output):
    global gradients
    gradients = grad_output[0]

target_layer.register_forward_hook(forward_hook)
target_layer.register_full_backward_hook(backward_hook)

# ----------------------------
# 5. Process Images
# ----------------------------
for img_name in os.listdir(input_folder):

    if not img_name.lower().endswith((".png", ".jpg", ".jpeg")):
        continue

    img_path = os.path.join(input_folder, img_name)
    img = Image.open(img_path).convert("RGB")

    input_tensor = transform(img).unsqueeze(0).to(device)

    # Forward + Backward
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

    model.zero_grad()
    output[0, pred_class].backward()

    # ----------------------------
    # 6. Grad-CAM
    # ----------------------------
    weights = gradients.mean(dim=(2, 3), keepdim=True)
    cam = (weights * activations).sum(dim=1).squeeze()
    cam = F.relu(cam)

    cam = cam.detach().cpu().numpy()
    cam = cv2.resize(cam, (224, 224))
    cam = cam / (cam.max() + 1e-8)

    # ----------------------------
    # 7. Prepare Images for Display
    # ----------------------------
    img_np = np.array(img.resize((224, 224))) / 255.0

    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = heatmap[:, :, ::-1] / 255.0  # BGR → RGB

    overlay = 0.6 * img_np + 0.4 * heatmap
    overlay = np.clip(overlay, 0, 1)

    # ----------------------------
    # 8. DISPLAY
    # ----------------------------
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.imshow(img_np)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(cam, cmap="jet")
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(overlay)
    plt.title("Grad-CAM Overlay")
    plt.axis("off")

    plt.suptitle(f"Image: {img_name}", fontsize=12)
    plt.tight_layout()
    plt.show()